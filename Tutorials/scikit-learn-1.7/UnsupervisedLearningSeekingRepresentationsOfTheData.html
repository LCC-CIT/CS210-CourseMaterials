<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>UnsupervisedLearningSeekingRepresentationsOfTheData</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<p><a href="ScikitLearnTutorials.html">scikit-learn tutorials
index</a></p>
<h1
id="unsupervised-learning-seeking-representations-of-the-data">Unsupervised
learning: seeking representations of the data</h1>
<h2 id="clustering-grouping-observations-together">Clustering: grouping
observations together</h2>
<p>The problem solved in clustering</p>
<p>Given the iris dataset, if we knew that there were 3 types of iris,
but did not have access to a taxonomist to label them: we could try a
<strong>clustering task</strong>: split the observations into
well-separated group called <em>clusters</em>.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Set the PRNG</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> np.random.seed(<span class="dv">1</span>)</span></code></pre></div>
<h3 id="k-means-clustering">K-means clustering</h3>
<p>Note that there exist a lot of different clustering criteria and
associated algorithms. The simplest clustering algorithm is <a
href="https://scikit-learn.org/1.7/modules/clustering.html#k-means" target="_blank">K-means</a>.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn <span class="im">import</span> cluster, datasets</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_iris, y_iris <span class="op">=</span> datasets.load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> k_means <span class="op">=</span> cluster.KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> k_means.fit(X_iris)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(k_means.labels_[::<span class="dv">10</span>])</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span>]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(y_iris[::<span class="dv">10</span>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span>]</span></code></pre></div>
<p><a
href="https://scikit-learn.org/1.7/auto_examples/cluster/plot_cluster_iris.html" target="_blank"><img
src="https://scikit-learn.org/1.7/_images/sphx_glr_plot_cluster_iris_001.png"
alt="../../_images/sphx_glr_plot_cluster_iris_001.png" /></a></p>
<p>Warning</p>
<p>There is absolutely no guarantee of recovering a ground truth. First,
choosing the right number of clusters is hard. Second, the algorithm is
sensitive to initialization, and can fall into local minima, although
scikit-learn employs several tricks to mitigate this issue.</p>
<p>For instance, on the image above, we can observe the difference
between the ground-truth (bottom right figure) and different clustering.
We do not recover the expected labels, either because the number of
cluster was chosen to be to large (top left figure) or suffer from a bad
initialization (bottom left figure).</p>
<p><strong>It is therefore important to not over-interpret clustering
results.</strong></p>
<p><strong>Application example: vector quantization</strong></p>
<p>Clustering in general and KMeans, in particular, can be seen as a way
of choosing a small number of exemplars to compress the information. The
problem is sometimes known as <a
href="https://en.wikipedia.org/wiki/Vector_quantization" target="_blank">vector
quantization</a>. For instance, this can be used to posterize an
image:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="cf">try</span>:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>...    face <span class="op">=</span> sp.face(gray<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>... <span class="cf">except</span> <span class="pp">AttributeError</span>:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>...    <span class="im">from</span> scipy <span class="im">import</span> misc</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>...    face <span class="op">=</span> misc.face(gray<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> face.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="co"># We need an (n_sample, n_feature) array</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> k_means <span class="op">=</span> cluster.KMeans(n_clusters<span class="op">=</span><span class="dv">5</span>, n_init<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> k_means.fit(X)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>KMeans(n_clusters<span class="op">=</span><span class="dv">5</span>, n_init<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> values <span class="op">=</span> k_means.cluster_centers_.squeeze()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> labels <span class="op">=</span> k_means.labels_</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> face_compressed <span class="op">=</span> np.choose(labels, values)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> face_compressed.shape <span class="op">=</span> face.shape</span></code></pre></div>
<p><strong>Raw image</strong></p>
<p><a
href="https://scikit-learn.org/1.7/auto_examples/cluster/plot_face_compress.html" target="_blank"><img
src="https://scikit-learn.org/1.7/_images/sphx_glr_plot_face_compress_001.png"
alt="../../_images/sphx_glr_plot_face_compress_001.png" /></a></p>
<p><strong>K-means quantization</strong></p>
<p><a
href="https://scikit-learn.org/1.7/auto_examples/cluster/plot_face_compress.html" target="_blank"><img
src="https://scikit-learn.org/1.7/_images/sphx_glr_plot_face_compress_004.png"
alt="../../_images/sphx_glr_plot_face_compress_004.png" /></a></p>
<p><strong>Equal bins</strong></p>
<p><a
href="https://scikit-learn.org/1.7/auto_examples/cluster/plot_face_compress.html" target="_blank"><img
src="https://scikit-learn.org/1.7/_images/sphx_glr_plot_face_compress_002.png"
alt="../../_images/sphx_glr_plot_face_compress_002.png" /></a></p>
<h3 id="hierarchical-agglomerative-clustering-ward">Hierarchical
agglomerative clustering: Ward</h3>
<p>A <a
href="https://scikit-learn.org/1.7/modules/clustering.html#hierarchical-clustering" target="_blank">Hierarchical
clustering</a> method is a type of cluster analysis that aims to build a
hierarchy of clusters. In general, the various approaches of this
technique are either:</p>
<ul>
<li><strong>Agglomerative</strong> - bottom-up approaches: each
observation starts in its own cluster, and clusters are iteratively
merged in such a way to minimize a <em>linkage</em> criterion. This
approach is particularly interesting when the clusters of interest are
made of only a few observations. When the number of clusters is large,
it is much more computationally efficient than k-means.</li>
<li><strong>Divisive</strong> - top-down approaches: all observations
start in one cluster, which is iteratively split as one moves down the
hierarchy. For estimating large numbers of clusters, this approach is
both slow (due to all observations starting as one cluster, which it
splits recursively) and statistically ill-posed.</li>
</ul>
<h4 id="connectivity-constrained-clustering">Connectivity-constrained
clustering</h4>
<p>With agglomerative clustering, it is possible to specify which
samples can be clustered together by giving a connectivity graph. Graphs
in scikit-learn are represented by their adjacency matrix. Often, a
sparse matrix is used. This can be useful, for instance, to retrieve
connected regions (sometimes also referred to as connected components)
when clustering an image.</p>
<p><a
href="https://scikit-learn.org/1.7/auto_examples/cluster/plot_coin_ward_segmentation.html" target="_blank"><img
src="https://scikit-learn.org/1.7/_images/sphx_glr_plot_coin_ward_segmentation_001.png"
alt="../../_images/sphx_glr_plot_coin_ward_segmentation_001.png" /></a></p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> skimage.data <span class="im">import</span> coins</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> scipy.ndimage <span class="im">import</span> gaussian_filter</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> skimage.transform <span class="im">import</span> rescale</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> rescaled_coins <span class="op">=</span> rescale(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>...     gaussian_filter(coins(), sigma<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>...     <span class="fl">0.2</span>, mode<span class="op">=</span><span class="st">&#39;reflect&#39;</span>, anti_aliasing<span class="op">=</span><span class="va">False</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>... )</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> np.reshape(rescaled_coins, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div>
<p>We need a vectorized version of the image.
<code>'rescaled_coins'</code> is a down-scaled version of the coins
image to speed up the process:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.feature_extraction <span class="im">import</span> grid_to_graph</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> connectivity <span class="op">=</span> grid_to_graph(<span class="op">*</span>rescaled_coins.shape)</span></code></pre></div>
<p>Define the graph structure of the data. Pixels connected to their
neighbors:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> n_clusters <span class="op">=</span> <span class="dv">27</span>  <span class="co"># number of regions</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> ward <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span>n_clusters, linkage<span class="op">=</span><span class="st">&#39;ward&#39;</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>...                                connectivity<span class="op">=</span>connectivity)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> ward.fit(X)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>AgglomerativeClustering(connectivity<span class="op">=</span>..., n_clusters<span class="op">=</span><span class="dv">27</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> label <span class="op">=</span> np.reshape(ward.labels_, rescaled_coins.shape)</span></code></pre></div>
<h4 id="feature-agglomeration">Feature agglomeration</h4>
<p>We have seen that sparsity could be used to mitigate the curse of
dimensionality, <em>i.e</em> an insufficient amount of observations
compared to the number of features. Another approach is to merge
together similar features: <strong>feature agglomeration</strong>. This
approach can be implemented by clustering in the feature direction, in
other words clustering the transposed data.</p>
<p><a
href="https://scikit-learn.org/1.7/auto_examples/cluster/plot_digits_agglomeration.html" target="_blank"><img
src="https://scikit-learn.org/1.7/_images/sphx_glr_plot_digits_agglomeration_001.png"
alt="../../_images/sphx_glr_plot_digits_agglomeration_001.png" /></a></p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> digits <span class="op">=</span> datasets.load_digits()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> images <span class="op">=</span> digits.images</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> np.reshape(images, (<span class="bu">len</span>(images), <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> connectivity <span class="op">=</span> grid_to_graph(<span class="op">*</span>images[<span class="dv">0</span>].shape)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> agglo <span class="op">=</span> cluster.FeatureAgglomeration(connectivity<span class="op">=</span>connectivity,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>...                                      n_clusters<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> agglo.fit(X)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>FeatureAgglomeration(connectivity<span class="op">=</span>..., n_clusters<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_reduced <span class="op">=</span> agglo.transform(X)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_approx <span class="op">=</span> agglo.inverse_transform(X_reduced)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> images_approx <span class="op">=</span> np.reshape(X_approx, images.shape)</span></code></pre></div>
<p><code>transform</code> and <code>inverse_transform</code> methods</p>
<p>Some estimators expose a <code>transform</code> method, for instance
to reduce the dimensionality of the dataset.</p>
<h2
id="decompositions-from-a-signal-to-components-and-loadings">Decompositions:
from a signal to components and loadings</h2>
<p><strong>Components and loadings</strong></p>
<p>If X is our multivariate data, then the problem that we are trying to
solve is to rewrite it on a different observational basis: we want to
learn loadings L and a set of components C such that <em>X = L C</em>.
Different criteria exist to choose the components</p>
<h3 id="principal-component-analysis-pca">Principal component analysis:
PCA</h3>
<p><a
href="https://scikit-learn.org/1.7/modules/decomposition.html#pca" target="_blank">Principal
component analysis (PCA)</a> selects the successive components that
explain the maximum variance in the signal. Let’s create a synthetic
3-dimensional dataset.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Create a signal with only 2 useful dimensions</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> x1 <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">1</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> x2 <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">1</span>))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> x3 <span class="op">=</span> x1 <span class="op">+</span> x2</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> np.concatenate([x1, x2, x3], axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<p>The point cloud spanned by the observations above is very flat in one
direction: one of the three univariate features (i.e. z-axis) can almost
be exactly computed using the other two.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> fig <span class="op">=</span> plt.figure()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">&#39;3d&#39;</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], X[:, <span class="dv">2</span>])</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>...<span class="op">&gt;</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> _ <span class="op">=</span> ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">&quot;x&quot;</span>, ylabel<span class="op">=</span><span class="st">&quot;y&quot;</span>, zlabel<span class="op">=</span><span class="st">&quot;z&quot;</span>)</span></code></pre></div>
<figure>
<img
src="https://scikit-learn.org/1.7/_images/unsupervised_learning-1.png"
alt="../../_images/unsupervised_learning-1.png" />
<figcaption
aria-hidden="true">../../_images/unsupervised_learning-1.png</figcaption>
</figure>
<p>PCA finds the directions in which the data is not <em>flat</em>.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn <span class="im">import</span> decomposition</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> pca <span class="op">=</span> decomposition.PCA()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> pca.fit(X)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>PCA()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(pca.explained_variance_)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>[  <span class="fl">2.18565811e+00</span>   <span class="fl">1.19346747e+00</span>   <span class="fl">8.43026679e-32</span>]</span></code></pre></div>
<p>Looking at the explained variance, we see that only the first two
components are useful. PCA can be used to reduce dimensionality while
preserving most of the information. It will project the data on the
principal subspace.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> pca.set_params(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_reduced <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_reduced.shape</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>(<span class="dv">100</span>, <span class="dv">2</span>)</span></code></pre></div>
<h3 id="independent-component-analysis-ica">Independent Component
Analysis: ICA</h3>
<p><a
href="https://scikit-learn.org/1.7/modules/decomposition.html#ica" target="_blank">Independent
component analysis (ICA)</a> selects components so that the distribution
of their loadings carries a maximum amount of independent information.
It is able to recover <strong>non-Gaussian</strong> independent
signals:</p>
<p><a
href="https://scikit-learn.org/1.7/auto_examples/decomposition/plot_ica_blind_source_separation.html" target="_blank"><img
src="https://scikit-learn.org/1.7/_images/sphx_glr_plot_ica_blind_source_separation_001.png"
alt="../../_images/sphx_glr_plot_ica_blind_source_separation_001.png" /></a></p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Generate sample data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> scipy <span class="im">import</span> signal</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> time <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">2000</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> s1 <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> time)  <span class="co"># Signal 1 : sinusoidal signal</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> s2 <span class="op">=</span> np.sign(np.sin(<span class="dv">3</span> <span class="op">*</span> time))  <span class="co"># Signal 2 : square signal</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> s3 <span class="op">=</span> signal.sawtooth(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> time)  <span class="co"># Signal 3: saw tooth signal</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> S <span class="op">=</span> np.c_[s1, s2, s3]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> S <span class="op">+=</span> <span class="fl">0.2</span> <span class="op">*</span> np.random.normal(size<span class="op">=</span>S.shape)  <span class="co"># Add noise</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> S <span class="op">/=</span> S.std(axis<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Standardize data</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Mix data</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="fl">0.5</span>, <span class="dv">2</span>, <span class="dv">1</span>], [<span class="fl">1.5</span>, <span class="dv">1</span>, <span class="dv">2</span>]])  <span class="co"># Mixing matrix</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> np.dot(S, A.T)  <span class="co"># Generate observations</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Compute ICA</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> ica <span class="op">=</span> decomposition.FastICA()</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> S_ <span class="op">=</span> ica.fit_transform(X)  <span class="co"># Get the estimated sources</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> A_ <span class="op">=</span> ica.mixing_.T</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> np.allclose(X,  np.dot(S_, A_) <span class="op">+</span> ica.mean_)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="va">True</span></span></code></pre></div>
<h2
id="enhanced-unsupervised-learning-features-in-scikit-learn-1.7">Enhanced
Unsupervised Learning Features in scikit-learn 1.7</h2>
<p>Since this tutorial was originally written for scikit-learn 1.4,
several enhancements have been made to unsupervised learning
capabilities:</p>
<h3 id="enhanced-array-api-support-for-unsupervised-learning">Enhanced
Array API Support for Unsupervised Learning</h3>
<p>scikit-learn 1.7 now supports Array API-compliant inputs in
unsupervised learning algorithms, making it easier to work with data
from libraries like PyTorch and CuPy:</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Works with PyTorch tensors</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>], [<span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Clustering with PyTorch tensors</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> kmeans.fit(X)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Dimensionality reduction with PyTorch tensors</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_reduced <span class="op">=</span> pca.fit_transform(X)</span></code></pre></div>
<h3 id="enhanced-sparse-data-support-in-unsupervised-learning">Enhanced
Sparse Data Support in Unsupervised Learning</h3>
<p>All unsupervised learning algorithms now support both traditional
sparse matrices (<code>scipy.sparse.spmatrix</code>) and the newer
sparse arrays (<code>scipy.sparse.sparray</code>):</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> scipy.sparse <span class="im">import</span> csr_array</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_sparse <span class="op">=</span> csr_array([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Clustering with sparse arrays</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> kmeans.fit(X_sparse)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Dimensionality reduction with sparse arrays</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svd <span class="op">=</span> TruncatedSVD(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_reduced <span class="op">=</span> svd.fit_transform(X_sparse)</span></code></pre></div>
<h3 id="enhanced-clustering-algorithms">Enhanced Clustering
Algorithms</h3>
<p>Several clustering algorithms have been improved with better
initialization strategies and performance optimizations:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans, AgglomerativeClustering</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X, _ <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">1000</span>, centers<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Improved K-means with better initialization</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, init<span class="op">=</span><span class="st">&#39;k-means++&#39;</span>, n_init<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> kmeans.fit(X)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Enhanced hierarchical clustering</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> agglo <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span><span class="st">&#39;ward&#39;</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> agglo.fit(X)</span></code></pre></div>
<h3 id="enhanced-dimensionality-reduction">Enhanced Dimensionality
Reduction</h3>
<p>PCA and other decomposition methods now have improved numerical
stability and support for more data types:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA, FastICA</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Generate sample data</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">10</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Enhanced PCA with better numerical stability</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">5</span>, svd_solver<span class="op">=</span><span class="st">&#39;auto&#39;</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_pca <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Improved FastICA</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> ica <span class="op">=</span> FastICA(n_components<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_ica <span class="op">=</span> ica.fit_transform(X)</span></code></pre></div>
<h3 id="enhanced-feature-agglomeration">Enhanced Feature
Agglomeration</h3>
<p>Feature agglomeration algorithms now support more connectivity
patterns and have improved performance:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.cluster <span class="im">import</span> FeatureAgglomeration</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.feature_extraction <span class="im">import</span> grid_to_graph</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Enhanced feature agglomeration</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> connectivity <span class="op">=</span> grid_to_graph(<span class="dv">8</span>, <span class="dv">8</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> agglo <span class="op">=</span> FeatureAgglomeration(n_clusters<span class="op">=</span><span class="dv">32</span>, connectivity<span class="op">=</span>connectivity)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_reduced <span class="op">=</span> agglo.fit_transform(X)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_restored <span class="op">=</span> agglo.inverse_transform(X_reduced)</span></code></pre></div>
<p>These enhancements maintain full backward compatibility while
providing more flexibility and power for unsupervised learning
tasks.</p>
<hr />
<p>This original version of this tutorial was written by scikit-learn
developers under the <a
href="https://opensource.org/license/BSD-3-clause" target="_blank">BSD License</a>.</p>
<hr />
<p>The code examples and text were updated for scikit-learn version 1.7
by Brian Bird using Claude Sonet 4, 10/19/2025</p>
<hr />
</body>
</html>
