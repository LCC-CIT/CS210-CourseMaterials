<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>SupervisedLearningPredictingAnOutputVariableFromHighDimensionalObservations</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<p><a href="ScikitLearnTutorials.html">scikit-learn tutorials
index</a></p>
<h1
id="supervised-learning-predicting-an-output-variable-from-high-dimensional-observations">Supervised
learning: predicting an output variable from high-dimensional
observations</h1>
<p>The problem solved in supervised learning</p>
<p><a
href="https://scikit-learn.org/1.7/supervised_learning.html#supervised-learning" target="_blank">Supervised
learning</a> consists in learning the link between two datasets: the
observed data <code>X</code> and an external variable <code>y</code>
that we are trying to predict, usually called “target” or “labels”. Most
often, <code>y</code> is a 1D array of length
<code>n_samples</code>.</p>
<p>All supervised <a
href="https://en.wikipedia.org/wiki/Estimator" target="_blank">estimators</a> in
scikit-learn implement a <code>fit(X, y)</code> method to fit the model
and a <code>predict(X)</code> method that, given unlabeled observations
<code>X</code>, returns the predicted labels <code>y</code>.</p>
<p>Vocabulary: classification and regression</p>
<p>If the prediction task is to classify the observations in a set of
finite labels, in other words to “name” the objects observed, the task
is said to be a <strong>classification</strong> task. On the other hand,
if the goal is to predict a continuous target variable, it is said to be
a <strong>regression</strong> task.</p>
<p>When doing classification in scikit-learn, <code>y</code> is a vector
of integers or strings.</p>
<p>Note: See the <a
href="https://scikit-learn.org/1.7/tutorial/basic/tutorial.html#introduction" target="_blank">Introduction
to machine learning with scikit-learn Tutorial</a> for a quick
run-through on the basic machine learning vocabulary used within
scikit-learn.</p>
<h2 id="nearest-neighbor-and-the-curse-of-dimensionality">Nearest
neighbor and the curse of dimensionality</h2>
<p>Classifying irises:</p>
<p>The iris dataset is a classification task consisting in identifying 3
different types of irises (Setosa, Versicolour, and Virginica) from
their petal and sepal length and width:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> iris_X, iris_y <span class="op">=</span> datasets.load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> np.unique(iris_y)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>])</span></code></pre></div>
<figure>
<img src="sphx_glr_plot_iris_dataset_001.png" alt="Iris Dataset" />
<figcaption aria-hidden="true">Iris Dataset</figcaption>
</figure>
<h3 id="k-nearest-neighbors-classifier">k-Nearest neighbors
classifier</h3>
<p>The simplest possible classifier is the <a
href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm" target="_blank">nearest
neighbor</a>: given a new observation <code>X_test</code>, find in the
training set (i.e. the data used to train the estimator) the observation
with the closest feature vector. (Please see the <a
href="https://scikit-learn.org/1.7/modules/neighbors.html#neighbors" target="_blank">Nearest
Neighbors section</a> of the online Scikit-learn documentation for more
information about this type of classifier.)</p>
<p>Training set and testing set</p>
<p>While experimenting with any learning algorithm, it is important not
to test the prediction of an estimator on the data used to fit the
estimator as this would not be evaluating the performance of the
estimator on <strong>new data</strong>. This is why datasets are often
split into <em>train</em> and <em>test</em> data.</p>
<p><strong>KNN (k nearest neighbors) classification
example</strong>:</p>
<figure>
<img src="sphx_glr_plot_classification_001.png"
alt="Classification Example" />
<figcaption aria-hidden="true">Classification Example</figcaption>
</figure>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Split iris data in train and test data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># A random permutation, to split the data randomly</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> indices <span class="op">=</span> np.random.permutation(<span class="bu">len</span>(iris_X))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> iris_X_train <span class="op">=</span> iris_X[indices[:<span class="op">-</span><span class="dv">10</span>]]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> iris_y_train <span class="op">=</span> iris_y[indices[:<span class="op">-</span><span class="dv">10</span>]]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> iris_X_test <span class="op">=</span> iris_X[indices[<span class="op">-</span><span class="dv">10</span>:]]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> iris_y_test <span class="op">=</span> iris_y[indices[<span class="op">-</span><span class="dv">10</span>:]]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Create and fit a nearest-neighbor classifier</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> knn <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> knn.fit(iris_X_train, iris_y_train)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>KNeighborsClassifier()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> knn.predict(iris_X_test)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>])</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> iris_y_test</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>])</span></code></pre></div>
<h3 id="the-curse-of-dimensionality">The curse of dimensionality</h3>
<p>For an estimator to be effective, you need the distance between
neighboring points to be less than some value</p>
<p>, which depends on the problem. In one dimension, this requires on
average points. In the context of the above -NN example, if the data is
described by just one feature with values ranging from 0 to 1 and with
training observations, then new data will be no further away than .
Therefore, the nearest neighbor decision rule will be efficient as soon
as</p>
<p>is small compared to the scale of between-class feature
variations.</p>
<p>If the number of features is</p>
<p>, you now require points. Let’s say that we require 10 points in one
dimension: now points are required in dimensions to pave the space.
As</p>
<p>becomes large, the number of training points required for a good
estimator grows exponentially.</p>
<p>For example, if each point is just a single number (8 bytes), then an
effective</p>
<p>-NN estimator in a paltry</p>
<p>dimensions would require more training data than the current
estimated size of the entire internet (±1000 Exabytes or so).</p>
<p>This is called the <a
href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank">curse of
dimensionality</a> and is a core problem that machine learning
addresses.</p>
<h2 id="linear-model-from-regression-to-sparsity">Linear model: from
regression to sparsity</h2>
<p>Diabetes dataset</p>
<p>The diabetes dataset consists of 10 physiological variables (age,
sex, weight, blood pressure) measured on 442 patients, and an indication
of disease progression after one year:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> diabetes_X, diabetes_y <span class="op">=</span> datasets.load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> diabetes_X_train <span class="op">=</span> diabetes_X[:<span class="op">-</span><span class="dv">20</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> diabetes_X_test  <span class="op">=</span> diabetes_X[<span class="op">-</span><span class="dv">20</span>:]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> diabetes_y_train <span class="op">=</span> diabetes_y[:<span class="op">-</span><span class="dv">20</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> diabetes_y_test  <span class="op">=</span> diabetes_y[<span class="op">-</span><span class="dv">20</span>:]</span></code></pre></div>
<p>The task at hand is to predict disease progression from physiological
variables.</p>
<h3 id="linear-regression">Linear regression</h3>
<p><a
href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" target="_blank"><code>LinearRegression</code></a>,
in its simplest form, fits a linear model to the data set by adjusting a
set of parameters in order to make the sum of the squared residuals of
the model as small as possible.</p>
<dl>
<dt>Linear models:</dt>
<dd>
<p>data</p>
</dd>
<dd>
<p>target variable</p>
</dd>
<dd>
<p>Coefficients</p>
</dd>
</dl>
<ul>
<li>: Observation noise</li>
</ul>
<figure>
<img src="sphx_glr_plot_ols_001.png" alt="OLS Regression" />
<figcaption aria-hidden="true">OLS Regression</figcaption>
</figure>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr.fit(diabetes_X_train, diabetes_y_train)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>LinearRegression()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(regr.coef_)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>[   <span class="fl">0.30349955</span> <span class="op">-</span><span class="fl">237.63931533</span>  <span class="fl">510.53060544</span>  <span class="fl">327.73698041</span> <span class="op">-</span><span class="fl">814.13170937</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="fl">492.81458798</span>  <span class="fl">102.84845219</span>  <span class="fl">184.60648906</span>  <span class="fl">743.51961675</span>   <span class="fl">76.09517222</span>]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># The mean square error</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> np.mean((regr.predict(diabetes_X_test) <span class="op">-</span> diabetes_y_test)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="fl">2004.5</span>...</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Explained variance score: 1 is perfect prediction</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># and 0 means that there is no linear relationship</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># between X and y.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr.score(diabetes_X_test, diabetes_y_test)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="fl">0.585</span>...</span></code></pre></div>
<h3 id="shrinkage">Shrinkage</h3>
<p>If there are few data points per dimension, noise in the observations
induces high variance:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> np.c_[ <span class="fl">.5</span>, <span class="dv">1</span>].T</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> [<span class="fl">.5</span>, <span class="dv">1</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> test <span class="op">=</span> np.c_[ <span class="dv">0</span>, <span class="dv">2</span>].T</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> plt.figure()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>...<span class="op">&gt;</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> np.random.seed(<span class="dv">0</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>...     this_X <span class="op">=</span> <span class="fl">.1</span> <span class="op">*</span> np.random.normal(size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>)) <span class="op">+</span> X</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>...     regr.fit(this_X, y)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>...     plt.plot(test, regr.predict(test))</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>...     plt.scatter(this_X, y, s<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>LinearRegression...</span></code></pre></div>
<figure>
<img src="sphx_glr_plot_ols_ridge_variance_001.png"
alt="Ridge Variance 1" />
<figcaption aria-hidden="true">Ridge Variance 1</figcaption>
</figure>
<p>A solution in high-dimensional statistical learning is to
<em>shrink</em> the regression coefficients to zero: any two randomly
chosen set of observations are likely to be uncorrelated. This is called
<a
href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" target="_blank"><code>Ridge</code></a>
regression:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr <span class="op">=</span> linear_model.Ridge(alpha<span class="op">=</span><span class="fl">.1</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> plt.figure()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>...<span class="op">&gt;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> np.random.seed(<span class="dv">0</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>...     this_X <span class="op">=</span> <span class="fl">.1</span> <span class="op">*</span> np.random.normal(size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>)) <span class="op">+</span> X</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>...     regr.fit(this_X, y)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>...     plt.plot(test, regr.predict(test))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>...     plt.scatter(this_X, y, s<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>Ridge...</span></code></pre></div>
<figure>
<img src="sphx_glr_plot_ols_ridge_variance_002.png"
alt="Ridge Variance 2" />
<figcaption aria-hidden="true">Ridge Variance 2</figcaption>
</figure>
<p>This is an example of <strong>bias/variance tradeoff</strong>: the
larger the ridge <code>alpha</code> parameter, the higher the bias and
the lower the variance.</p>
<p>We can choose <code>alpha</code> to minimize left out error, this
time using the diabetes dataset rather than our synthetic data:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> alphas <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>([regr.set_params(alpha<span class="op">=</span>alpha)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>...            .fit(diabetes_X_train, diabetes_y_train)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>...            .score(diabetes_X_test, diabetes_y_test)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>...        <span class="cf">for</span> alpha <span class="kw">in</span> alphas])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>[<span class="fl">0.585</span>..., <span class="fl">0.585</span>..., <span class="fl">0.5854</span>..., <span class="fl">0.5855</span>..., <span class="fl">0.583</span>..., <span class="fl">0.570</span>...]</span></code></pre></div>
<p>Note</p>
<p>Capturing in the fitted parameters noise that prevents the model to
generalize to new data is called <a
href="https://en.wikipedia.org/wiki/Overfitting" target="_blank">overfitting</a>. The
bias introduced by the ridge regression is called a <a
href="https://en.wikipedia.org/wiki/Regularization_(machine_learning)" target="_blank">regularization</a>.</p>
<h3 id="sparsity">Sparsity</h3>
<p><strong>Fitting only features 1 and 2</strong></p>
<p><strong><img src="sphx_glr_plot_ols_3d_001.png"
alt="Diabetes OLS 1" /> <img src="sphx_glr_plot_ols_3d_003.png"
alt="Diabetes OLS 3" /> <img src="sphx_glr_plot_ols_3d_002.png"
alt="Diabetes OLS 2" /></strong></p>
<p>Note</p>
<p>A representation of the full diabetes dataset would involve 11
dimensions (10 feature dimensions and one of the target variable). It is
hard to develop an intuition on such representation, but it may be
useful to keep in mind that it would be a fairly <em>empty</em>
space.</p>
<p>We can see that, although feature 2 has a strong coefficient on the
full model, it conveys little information on <code>y</code> when
considered with feature 1.</p>
<p>To improve the conditioning of the problem (i.e. mitigating the <a
href="https://scikit-learn.org/1.7/tutorial/statistical_inference/supervised_learning.html#curse-of-dimensionality" target="_blank">The
curse of dimensionality</a>), it would be interesting to select only the
informative features and set non-informative ones, like feature 2 to 0.
Ridge regression will decrease their contribution, but not set them to
zero. Another penalization approach, called <a
href="https://scikit-learn.org/1.7/modules/linear_model.html#lasso" target="_blank">Lasso</a>
(least absolute shrinkage and selection operator), can set some
coefficients to zero. Such methods are called <strong>sparse
methods</strong> and sparsity can be seen as an application of Occam’s
razor: <em>prefer simpler models</em>.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr <span class="op">=</span> linear_model.Lasso()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> scores <span class="op">=</span> [regr.set_params(alpha<span class="op">=</span>alpha)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>...               .fit(diabetes_X_train, diabetes_y_train)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>...               .score(diabetes_X_test, diabetes_y_test)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>...           <span class="cf">for</span> alpha <span class="kw">in</span> alphas]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> best_alpha <span class="op">=</span> alphas[scores.index(<span class="bu">max</span>(scores))]</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr.alpha <span class="op">=</span> best_alpha</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> regr.fit(diabetes_X_train, diabetes_y_train)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>Lasso(alpha<span class="op">=</span><span class="fl">0.025118864315095794</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(regr.coef_)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>[   <span class="fl">0.</span>         <span class="op">-</span><span class="fl">212.4</span>...   <span class="fl">517.2</span>...  <span class="fl">313.7</span>... <span class="op">-</span><span class="fl">160.8</span>...</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>   <span class="op">-</span><span class="fl">0.</span>         <span class="op">-</span><span class="fl">187.1</span>...   <span class="fl">69.3</span>...  <span class="fl">508.6</span>...   <span class="fl">71.8</span>... ]</span></code></pre></div>
<p><strong>Different algorithms for the same problem</strong></p>
<p>Different algorithms can be used to solve the same mathematical
problem. For instance the <code>Lasso</code> object in scikit-learn
solves the lasso regression problem using a <a
href="https://en.wikipedia.org/wiki/Coordinate_descent" target="_blank">coordinate
descent</a> method, that is efficient on large datasets. However,
scikit-learn also provides the <a
href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars" target="_blank"><code>LassoLars</code></a>
object using the <em>LARS</em> algorithm, which is very efficient for
problems in which the weight vector estimated is very sparse
(i.e. problems with very few observations).</p>
<h3 id="classification">Classification</h3>
<p>For classification, as in the labeling <a
href="https://en.wikipedia.org/wiki/Iris_flower_data_set" target="_blank">iris</a> task,
linear regression is not the right approach as it will give too much
weight to data far from the decision frontier. A linear approach is to
fit a sigmoid function or <strong>logistic</strong> function:</p>
<figure>
<img src="sphx_glr_plot_logistic_001.png" alt="Logistic Regression" />
<figcaption aria-hidden="true">Logistic Regression</figcaption>
</figure>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> log <span class="op">=</span> linear_model.LogisticRegression(C<span class="op">=</span><span class="fl">1e5</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> log.fit(iris_X_train, iris_y_train)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>LogisticRegression(C<span class="op">=</span><span class="fl">100000.0</span>)</span></code></pre></div>
<p>This is known as <a
href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank"><code>LogisticRegression</code></a>.</p>
<figure>
<img src="sphx_glr_plot_iris_logistic_001.png" alt="Iris Logistic" />
<figcaption aria-hidden="true">Iris Logistic</figcaption>
</figure>
<p>Multiclass classification</p>
<p>If you have several classes to predict, an option often used is to
fit one-versus-all classifiers and then use a voting heuristic for the
final decision.</p>
<p>Shrinkage and sparsity with logistic regression</p>
<p>The <code>C</code> parameter controls the amount of regularization in
the <a
href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank"><code>LogisticRegression</code></a>
object: a large value for <code>C</code> results in less regularization.
<code>penalty="l2"</code> gives <a
href="https://scikit-learn.org/1.7/tutorial/statistical_inference/supervised_learning.html#shrinkage" target="_blank">Shrinkage</a>
(i.e. non-sparse coefficients), while <code>penalty="l1"</code> gives <a
href="https://scikit-learn.org/1.7/tutorial/statistical_inference/supervised_learning.html#sparsity" target="_blank">Sparsity</a>.</p>
<p><strong>Exercise</strong></p>
<p>Try classifying the digits dataset with nearest neighbors and a
linear model. Leave out the last 10% and test prediction performance on
these observations.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets, linear_model, neighbors</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>X_digits, y_digits <span class="op">=</span> datasets.load_digits(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>X_digits <span class="op">=</span> X_digits <span class="op">/</span> X_digits.<span class="bu">max</span>()</span></code></pre></div>
<p>A solution can be downloaded <a
href="https://scikit-learn.org/1.7/_downloads/e4d278c5c3a8450d66b5dd01a57ae923/plot_digits_classification_exercise.py" target="_blank"><code>here</code></a>.</p>
<h2 id="support-vector-machines-svms">Support vector machines
(SVMs)</h2>
<h3 id="linear-svms">Linear SVMs</h3>
<p><a href="https://scikit-learn.org/1.7/modules/svm.html#svm" target="_blank">Support
Vector Machines</a> belong to the discriminant model family: they try to
find a combination of samples to build a plane maximizing the margin
between the two classes. Regularization is set by the <code>C</code>
parameter: a small value for <code>C</code> means the margin is
calculated using many or all of the observations around the separating
line (more regularization); a large value for <code>C</code> means the
margin is calculated on observations close to the separating line (less
regularization).</p>
<figure>
<img src="sphx_glr_plot_svm_margin_001.png" alt="SVM Margin 1" />
<figcaption aria-hidden="true">SVM Margin 1</figcaption>
</figure>
<p><strong>Unregularized SVM</strong></p>
<figure>
<img src="sphx_glr_plot_svm_margin_002.png" alt="SVM Margin 2" />
<figcaption aria-hidden="true">SVM Margin 2</figcaption>
</figure>
<p><strong>Regularized SVM (default)</strong></p>
<p>Example:</p>
<ul>
<li><a
href="https://scikit-learn.org/1.7/auto_examples/svm/plot_iris_svc.html#sphx-glr-auto-examples-svm-plot-iris-svc-py" target="_blank">Plot
different SVM classifiers in the iris dataset</a></li>
</ul>
<p>SVMs can be used in regression –<a
href="https://scikit-learn.org/1.7/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR" target="_blank"><code>SVR</code></a>
(Support Vector Regression)–, or in classification –<a
href="https://scikit-learn.org/1.7/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" target="_blank"><code>SVC</code></a>
(Support Vector Classification).</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svc <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svc.fit(iris_X_train, iris_y_train)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>SVC(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>)</span></code></pre></div>
<p>Warning</p>
<p><strong>Normalizing data</strong></p>
<p>For many estimators, including the SVMs, having datasets with unit
standard deviation for each feature is important to get good
prediction.</p>
<h3 id="using-kernels">Using kernels</h3>
<p>Classes are not always linearly separable in feature space. The
solution is to build a decision function that is not linear but may be
polynomial instead. This is done using the <em>kernel trick</em> that
can be seen as creating a decision energy by positioning
<em>kernels</em> on observations:</p>
<h4 id="linear-kernel">Linear kernel</h4>
<figure>
<img src="sphx_glr_plot_svm_kernels_002.png" alt="SVM Kernels 2" />
<figcaption aria-hidden="true">SVM Kernels 2</figcaption>
</figure>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svc <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>)</span></code></pre></div>
<h4 id="polynomial-kernel">Polynomial kernel</h4>
<figure>
<img src="sphx_glr_plot_svm_kernels_003.png" alt="SVM Kernels 3" />
<figcaption aria-hidden="true">SVM Kernels 3</figcaption>
</figure>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svc <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">&#39;poly&#39;</span>,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>...               degree<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># degree: polynomial degree</span></span></code></pre></div>
<h4 id="rbf-kernel-radial-basis-function">RBF kernel (Radial Basis
Function)</h4>
<figure>
<img src="sphx_glr_plot_svm_kernels_004.png" alt="SVM Kernels 4" />
<figcaption aria-hidden="true">SVM Kernels 4</figcaption>
</figure>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svc <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">&#39;rbf&#39;</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># gamma: inverse of size of</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># radial kernel</span></span></code></pre></div>
<h4 id="sigmoid-kernel">Sigmoid kernel</h4>
<figure>
<img src="sphx_glr_plot_svm_kernels_005.png" alt="SVM Kernels 5" />
<figcaption aria-hidden="true">SVM Kernels 5</figcaption>
</figure>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svc <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)</span></code></pre></div>
<p><strong>Interactive example</strong></p>
<p>See the <a
href="https://scikit-learn.org/1.7/auto_examples/applications/svm_gui.html#sphx-glr-auto-examples-applications-svm-gui-py" target="_blank">SVM
GUI</a> to download <code>svm_gui.py</code>; add data points of both
classes with right and left button, fit the model and change parameters
and data.</p>
<p><strong>Exercise</strong></p>
<p>Try classifying classes 1 and 2 from the iris dataset with SVMs, with
the 2 first features. Leave out 10% of each class and test prediction
performance on these observations.</p>
<p><strong>Warning</strong>: the classes are ordered, do not leave out
the last 10%, you would be testing on only one class.</p>
<p><strong>Hint</strong>: You can use the <code>decision_function</code>
method on a grid to get intuitions.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris.data</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> iris.target</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X[y <span class="op">!=</span> <span class="dv">0</span>, :<span class="dv">2</span>]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y[y <span class="op">!=</span> <span class="dv">0</span>]</span></code></pre></div>
<figure>
<img src="sphx_glr_plot_iris_dataset_001.png" alt="Iris Dataset" />
<figcaption aria-hidden="true">Iris Dataset</figcaption>
</figure>
<p>A solution can be downloaded <a
href="https://scikit-learn.org/1.7/_downloads/a3ad6892094cf4c9641b7b11f9263348/plot_iris_exercise.py" target="_blank"><code>here</code></a></p>
<h2
id="enhanced-supervised-learning-features-in-scikit-learn-1.7">Enhanced
Supervised Learning Features in scikit-learn 1.7</h2>
<p>Since this tutorial was originally written for scikit-learn 1.4,
several enhancements have been made to supervised learning
capabilities:</p>
<h3 id="enhanced-array-api-support">Enhanced Array API Support</h3>
<p>scikit-learn 1.7 now supports Array API-compliant inputs, making it
easier to work with data from libraries like PyTorch and CuPy directly
in supervised learning tasks:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> torch</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Works with PyTorch tensors</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> torch.tensor([<span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">11</span>])</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> reg <span class="op">=</span> LinearRegression()</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> reg.fit(X, y)  <span class="co"># Seamless integration with PyTorch</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Also works with classification</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_clf <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">5</span>]])</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> y_clf <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> clf <span class="op">=</span> SVC()</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> clf.fit(X_clf, y_clf)</span></code></pre></div>
<h3 id="enhanced-sparse-data-support">Enhanced Sparse Data Support</h3>
<p>All supervised learning estimators now support both traditional
sparse matrices (<code>scipy.sparse.spmatrix</code>) and the newer
sparse arrays (<code>scipy.sparse.sparray</code>):</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> scipy.sparse <span class="im">import</span> csr_array</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_sparse <span class="op">=</span> csr_array([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Works with both sparse formats</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> log_reg <span class="op">=</span> LogisticRegression()</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> log_reg.fit(X_sparse, y)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svm_clf <span class="op">=</span> SVC()</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> svm_clf.fit(X_sparse, y)</span></code></pre></div>
<h3 id="enhanced-gradient-boosting-for-supervised-learning">Enhanced
Gradient Boosting for Supervised Learning</h3>
<p><code>HistGradientBoostingClassifier</code> and
<code>HistGradientBoostingRegressor</code> now support explicit
validation sets for better early stopping and overfitting control:</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.ensemble <span class="im">import</span> HistGradientBoostingClassifier</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> clf <span class="op">=</span> HistGradientBoostingClassifier(enable_metadata_routing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> clf.fit(X_train, y_train, X_val<span class="op">=</span>X_val, y_val<span class="op">=</span>y_val)</span></code></pre></div>
<h3 id="enhanced-multilayer-perceptron">Enhanced Multilayer
Perceptron</h3>
<p>The Multilayer Perceptron now supports Poisson loss (useful for count
data) and sample weights, enhancing its flexibility for various
supervised learning applications:</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPRegressor</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Poisson loss for count data</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> mlp_poisson <span class="op">=</span> MLPRegressor(loss<span class="op">=</span><span class="st">&#39;poisson&#39;</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> mlp_poisson.fit(X_train, y_train)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="co"># Sample weights support</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> sample_weights <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">1.5</span>, <span class="fl">0.8</span>])</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> mlp_weighted <span class="op">=</span> MLPRegressor()</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> mlp_weighted.fit(X_train, y_train, sample_weight<span class="op">=</span>sample_weights)</span></code></pre></div>
<p>These enhancements maintain full backward compatibility while
providing more flexibility and power for supervised learning tasks.</p>
<hr />
<p>This original version of this tutorial was written by scikit-learn
developers under the <a
href="https://opensource.org/license/BSD-3-clause" target="_blank">BSD License</a>.</p>
<hr />
<p>The code examples and text were updated for scikit-learn version 1.7
by Brian Bird using Claude Sonet 4, 10/19/2025</p>
<hr />
</body>
</html>
